{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "",
  "signature": "sha256:4184a2e5a57e637ebda5d54a4589efa47901468250d418f67daf0c235997cf93"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import matplotlib\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline  \n",
      "\n",
      "import calendar\n",
      "\n",
      "import sklearn\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn import linear_model\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import mean_absolute_error as mae\n",
      "from sklearn.ensemble import GradientBoostingRegressor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
        "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = pd.read_csv(\"train.csv\")\n",
      "test = pd.read_csv(\"test.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = train.columns.difference(['City'])\n",
      "trainNew = pd.get_dummies(train, columns=['City'])\n",
      "print train.shape\n",
      "print trainNew.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(46048, 6)\n",
        "(46048, 119)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#normData\n",
      "data = train.copy()\n",
      "\n",
      "\n",
      "c = data.columns.difference(['Count'])\n",
      "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(data[c], data['Count'], test_size=0.3, random_state=0)\n",
      "\n",
      "\n",
      "#pred = map(round, pred)\n",
      "#print \"result \", mae(pred, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[data.City == \"\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1\" & data.]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Day</th>\n",
        "      <th>Month</th>\n",
        "      <th>Year</th>\n",
        "      <th>City</th>\n",
        "      <th>Count</th>\n",
        "      <th>WeekDay</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>8</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>42</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51</th>\n",
        "      <td>7</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>21</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>180</th>\n",
        "      <td>7</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>35</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1309</th>\n",
        "      <td>15</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>41</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1698</th>\n",
        "      <td>17</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>32</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1836</th>\n",
        "      <td>6</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>16</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2133</th>\n",
        "      <td>24</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>54</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2990</th>\n",
        "      <td>1</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>42</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3792</th>\n",
        "      <td>3</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>32</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4225</th>\n",
        "      <td>18</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>17</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4411</th>\n",
        "      <td>5</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>26</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5275</th>\n",
        "      <td>23</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>39</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5617</th>\n",
        "      <td>22</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>30</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5680</th>\n",
        "      <td>20</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>25</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5936</th>\n",
        "      <td>10</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>54</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6659</th>\n",
        "      <td>14</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>17</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6669</th>\n",
        "      <td>4</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>40</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6948</th>\n",
        "      <td>9</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>37</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7264</th>\n",
        "      <td>28</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>57</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8181</th>\n",
        "      <td>15</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>17</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8302</th>\n",
        "      <td>24</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>40</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11255</th>\n",
        "      <td>21</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>27</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11396</th>\n",
        "      <td>12</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>34</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11571</th>\n",
        "      <td>12</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>33</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11957</th>\n",
        "      <td>12</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>53</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12894</th>\n",
        "      <td>25</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>43</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13491</th>\n",
        "      <td>14</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>56</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13524</th>\n",
        "      <td>29</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>28</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13667</th>\n",
        "      <td>1</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>41</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13670</th>\n",
        "      <td>14</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>25</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27004</th>\n",
        "      <td>10</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>44</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27256</th>\n",
        "      <td>23</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>44</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27294</th>\n",
        "      <td>30</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>71</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27917</th>\n",
        "      <td>8</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>47</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27958</th>\n",
        "      <td>11</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>36</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28072</th>\n",
        "      <td>19</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>59</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28256</th>\n",
        "      <td>9</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>35</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29005</th>\n",
        "      <td>6</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>63</td>\n",
        "      <td>5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29336</th>\n",
        "      <td>2</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>44</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29599</th>\n",
        "      <td>16</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>35</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30123</th>\n",
        "      <td>16</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>65</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30143</th>\n",
        "      <td>20</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>57</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30346</th>\n",
        "      <td>29</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>59</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30369</th>\n",
        "      <td>13</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>37</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32465</th>\n",
        "      <td>19</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>58</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32878</th>\n",
        "      <td>24</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>46</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33423</th>\n",
        "      <td>19</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>18</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35169</th>\n",
        "      <td>3</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>11</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36409</th>\n",
        "      <td>30</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>71</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37671</th>\n",
        "      <td>4</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>22</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38461</th>\n",
        "      <td>22</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>32</td>\n",
        "      <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39171</th>\n",
        "      <td>26</td>\n",
        "      <td>7</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>58</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39319</th>\n",
        "      <td>23</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>51</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39422</th>\n",
        "      <td>22</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>51</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39628</th>\n",
        "      <td>5</td>\n",
        "      <td>10</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>30</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39984</th>\n",
        "      <td>31</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>32</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40329</th>\n",
        "      <td>6</td>\n",
        "      <td>9</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>30</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40444</th>\n",
        "      <td>3</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>43</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41837</th>\n",
        "      <td>11</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>34</td>\n",
        "      <td>3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41995</th>\n",
        "      <td>21</td>\n",
        "      <td>8</td>\n",
        "      <td>2016</td>\n",
        "      <td>\u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1</td>\n",
        "      <td>49</td>\n",
        "      <td>6</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>87 rows \u00d7 6 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "       Day  Month  Year           City  Count  WeekDay\n",
        "0        8      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     42        0\n",
        "51       7     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     21        4\n",
        "180      7      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     35        6\n",
        "1309    15      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     41        0\n",
        "1698    17     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     32        0\n",
        "1836     6     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     16        3\n",
        "2133    24      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     54        6\n",
        "2990     1     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     42        5\n",
        "3792     3      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     32        5\n",
        "4225    18     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     17        1\n",
        "4411     5      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     26        0\n",
        "5275    23      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     39        4\n",
        "5617    22      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     30        3\n",
        "5680    20      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     25        1\n",
        "5936    10      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     54        5\n",
        "6659    14      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     17        2\n",
        "6669     4      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     40        3\n",
        "6948     9     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     37        6\n",
        "7264    28      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     57        3\n",
        "8181    15      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     17        3\n",
        "8302    24      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     40        5\n",
        "11255   21      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     27        3\n",
        "11396   12      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     34        0\n",
        "11571   12     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     33        2\n",
        "11957   12      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     53        4\n",
        "12894   25      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     43        3\n",
        "13491   14      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     56        6\n",
        "13524   29      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     28        3\n",
        "13667    1      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     41        0\n",
        "13670   14     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     25        4\n",
        "...    ...    ...   ...            ...    ...      ...\n",
        "27004   10      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     44        2\n",
        "27256   23      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     44        5\n",
        "27294   30      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     71        5\n",
        "27917    8     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     47        5\n",
        "27958   11      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     36        6\n",
        "28072   19      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     59        4\n",
        "28256    9      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     35        4\n",
        "29005    6      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     63        5\n",
        "29336    2      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     44        1\n",
        "29599   16      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     35        4\n",
        "30123   16      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     65        1\n",
        "30143   20      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     57        2\n",
        "30346   29      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     59        4\n",
        "30369   13     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     37        3\n",
        "32465   19      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     58        1\n",
        "32878   24      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     46        2\n",
        "33423   19      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     18        0\n",
        "35169    3     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     11        0\n",
        "36409   30      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     71        1\n",
        "37671    4     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     22        1\n",
        "38461   22      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     32        4\n",
        "39171   26      7  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     58        1\n",
        "39319   23      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     51        1\n",
        "39422   22      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     51        0\n",
        "39628    5     10  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     30        2\n",
        "39984   31      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     32        2\n",
        "40329    6      9  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     30        1\n",
        "40444    3      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     43        2\n",
        "41837   11      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     34        3\n",
        "41995   21      8  2016  \u041b\u0435\u043d\u0438\u043d\u043e\u0433\u043e\u0440\u0441\u043a-1     49        6\n",
        "\n",
        "[87 rows x 6 columns]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainModelKfold(data, model, countFold = 5):\n",
      "    meanMae = 0\n",
      "    \n",
      "    c = trainNew.columns.difference(['Count'])\n",
      "    countFold = 5\n",
      "    kf = KFold(len(trainNew), n_folds=countFold, shuffle=True)\n",
      "    \n",
      "    for train_index, test_index in kf:\n",
      "        X_train, X_test = data[c].loc[train_index], data[c].loc[test_index]\n",
      "        y_train, y_test = data['Count'].loc[train_index], data['Count'].loc[test_index]\n",
      "        model.fit(X_train.as_matrix(), y_train.as_matrix())\n",
      "        pred = model.predict(X_test)\n",
      "        print mae(pred, y_test)\n",
      "        meanMae += mae(pred, y_test)\n",
      "    meanMae /= countFold\n",
      "    print \"result \", meanMae"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainModelTestTrainSplit(data, model):\n",
      "    c = data.columns.difference(['Count'])\n",
      "    X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(data[c], data['Count'], test_size=0.3, random_state=0)\n",
      "    model.verbose=1\n",
      "    model.fit(X_train.as_matrix(), y_train.as_matrix())\n",
      "    pred = model.predict(X_test)\n",
      "    #pred = map(round, pred)\n",
      "    print \"result \", mae(pred, y_test)\n",
      "    return pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, LinearRegression())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result  30.3775054564\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#normData\n",
      "data = trainNew.copy()\n",
      "cols_to_norm = [\"Day\", \"Month\", \"Year\"]\n",
      "data[cols_to_norm] = data[cols_to_norm].apply(lambda x: (x - x.mean()) / (x.max() - x.min()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rgr = trainModelTestTrainSplit(trainNew, RandomForestRegressor())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.9s finished\n",
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result  19.4423307999\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rgr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([  66.9,   59.3,   77.6, ...,   80.3,   72.4,  103.1])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, LinearRegression())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result  30.3775054564\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, linear_model.ElasticNet())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result  52.2915733832\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, sklearn.ensemble.ExtraTreesRegressor(criterion=\"mae\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, sklearn.ensemble.RandomForestRegressor(n_estimators=300))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainModelTestTrainSplit(trainNew, sklearn.ensemble.BaggingRegressor())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result  19.6640101339\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = trainNew\n",
      "model = sklearn.ensemble.GradientBoostingRegressor(loss = 'ls', learning_rate=0.1, max_depth = 3, verbose=1, n_estimators=10,\\\n",
      "                                                   criterion = \"mae\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = trainNew.columns.difference(['Count'])\n",
      "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(data[c], data['Count'], test_size=0.3, random_state=0)\n",
      "model.fit(X_train.as_matrix(), y_train.as_matrix())\n",
      "pred = model.predict(X_test)\n",
      "print \"result \", mae(pred, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.8s finished\n",
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      Iter       Train Loss   Remaining Time \n"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "too many indices for array",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-cce71ee070b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainNew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"result \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1028\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    795\u001b[0m                 loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n\u001b[1;32m    796\u001b[0m                                              \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                                              self.learning_rate, k=k)\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# add tree to ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mupdate_terminal_regions\u001b[0;34m(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \"\"\"\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# update predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
        "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.feature_importances_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "array([  5.44335264e-03,   6.45594826e-03,   2.00477209e-03,\n",
        "         9.59723684e-03,   7.83662815e-03,   5.90436130e-04,\n",
        "         7.09270144e-03,   1.39095627e-03,   3.01235950e-03,\n",
        "         4.45238844e-03,   5.77847319e-03,   3.63852348e-04,\n",
        "         6.87055254e-03,   3.49814892e-04,   9.16518989e-03,\n",
        "         9.38886899e-03,   7.41757308e-03,   6.70804112e-03,\n",
        "         2.75003366e-03,   7.55657461e-03,   4.00109010e-03,\n",
        "         6.11474055e-03,   6.90328962e-03,   1.57537229e-03,\n",
        "         8.98828173e-04,   1.44579894e-03,   8.40689742e-03,\n",
        "         4.65505072e-03,   4.03495255e-03,   0.00000000e+00,\n",
        "         5.63729929e-04,   2.12886966e-03,   3.87871486e-03,\n",
        "         1.82087988e-03,   0.00000000e+00,   6.40451950e-03,\n",
        "         6.36515361e-03,   8.08304357e-03,   8.90893069e-03,\n",
        "         1.02533973e-03,   6.02295877e-04,   3.58951892e-03,\n",
        "         1.02209127e-02,   1.18976038e-04,   5.60662063e-03,\n",
        "         2.56260105e-03,   9.26532177e-03,   6.65150160e-03,\n",
        "         8.78266222e-03,   9.94361015e-03,   4.73715350e-05,\n",
        "         9.83143450e-03,   6.02141461e-03,   2.00874040e-03,\n",
        "         8.61441109e-04,   1.38048957e-02,   8.73582659e-04,\n",
        "         2.09189841e-03,   1.00614809e-02,   0.00000000e+00,\n",
        "         7.75694175e-04,   0.00000000e+00,   9.00755628e-03,\n",
        "         5.65705609e-03,   7.61523792e-03,   6.26977412e-03,\n",
        "         0.00000000e+00,   4.34424830e-03,   3.67875338e-03,\n",
        "         0.00000000e+00,   1.22122903e-02,   6.93098696e-03,\n",
        "         6.54584066e-03,   5.49807954e-03,   7.11604569e-03,\n",
        "         3.42946909e-03,   4.63263054e-03,   2.23978109e-03,\n",
        "         6.73101438e-03,   7.48706651e-03,   6.90282498e-03,\n",
        "         4.80281145e-03,   2.84736081e-03,   1.46459599e-03,\n",
        "         4.21856259e-03,   1.09692665e-02,   1.53139873e-03,\n",
        "         5.24616480e-03,   5.58731250e-03,   1.04745009e-02,\n",
        "         1.37546063e-02,   7.04337965e-03,   6.83524544e-03,\n",
        "         6.46186358e-03,   5.58284854e-03,   3.20346397e-03,\n",
        "         4.04436701e-03,   2.32113594e-03,   1.17243715e-03,\n",
        "         5.63758324e-03,   3.21506066e-03,   1.85739639e-03,\n",
        "         7.39162224e-03,   9.67425931e-03,   5.16739808e-03,\n",
        "         3.60384303e-03,   9.19960912e-03,   4.23040439e-03,\n",
        "         2.26111897e-03,   8.64476831e-03,   1.44670103e-02,\n",
        "         7.67034011e-03,   6.73984642e-03,   8.67579180e-04,\n",
        "         4.59501168e-02,   1.47179074e-01,   1.01291314e-01,\n",
        "         1.25928747e-01])"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}